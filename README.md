# ğŸ§  LLM Client App

A Flutter-based mobile app that acts as a client for accessing **Ollama** models running on your local PC server. Built as a **college project** to demonstrate hybrid mobile development and integration with local LLM backends.

---

## ğŸš€ Features

âœ… Connect to a local Ollama server via IP and port  
âœ… Fetch available LLM models from server  
âœ… Chat with the selected LLM model  
âœ… Simple, modern UI built with Flutter  
âœ… Hybrid design for Android/iOS support

---

## ğŸ“± Screenshots

### Chat Screen

<img width="366" height="556" alt="image" src="https://github.com/user-attachments/assets/12f3d4dd-20a4-4740-a8e6-76d0cc78fa7a" />



---

### Settings Screen
<img width="300" height="481" alt="image" src="https://github.com/user-attachments/assets/6328efc5-5372-4cbe-bfa8-bc58b74f733d" />


---

## âš™ï¸ How It Works

1ï¸âƒ£ Start your [Ollama server](https://ollama.com/) on your PC (ensure it's reachable on your local network).  
2ï¸âƒ£ Enter the server URL and port in the appâ€™s settings.  
3ï¸âƒ£ Fetch available models and select one.  
4ï¸âƒ£ Start chatting!

---

## ğŸ› ï¸ Tech Stack

- Flutter (Dart)
- Android/iOS support
- REST API calls to Ollama server
- Local network communication


