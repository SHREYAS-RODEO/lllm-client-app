# 🧠 LLM Client App

A Flutter-based mobile app that acts as a client for accessing **Ollama** models running on your local PC server. Built as a **college project** to demonstrate hybrid mobile development and integration with local LLM backends.

---

## 🚀 Features

✅ Connect to a local Ollama server via IP and port  
✅ Fetch available LLM models from server  
✅ Chat with the selected LLM model  
✅ Simple, modern UI built with Flutter  
✅ Hybrid design for Android/iOS support

---

## 📱 Screenshots

### Chat Screen

<img width="366" height="556" alt="image" src="https://github.com/user-attachments/assets/12f3d4dd-20a4-4740-a8e6-76d0cc78fa7a" />



---

### Settings Screen
<img width="300" height="481" alt="image" src="https://github.com/user-attachments/assets/6328efc5-5372-4cbe-bfa8-bc58b74f733d" />


---

## ⚙️ How It Works

1️⃣ Start your [Ollama server](https://ollama.com/) on your PC (ensure it's reachable on your local network).  
2️⃣ Enter the server URL and port in the app’s settings.  
3️⃣ Fetch available models and select one.  
4️⃣ Start chatting!

---

## 🛠️ Tech Stack

- Flutter (Dart)
- Android/iOS support
- REST API calls to Ollama server
- Local network communication


